{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqy6AtuYhknk"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U torch\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U datasets\n",
        "!pip install transformers==4.31\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U git+https://github.com/lvwerra/trl.git\n",
        "!pip install -q -U sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvSyBKJm-0n9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import bitsandbytes as bnb\n",
        "from functools import partial\n",
        "import os\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training,PeftConfig,PeftModel\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM,LlamaForCausalLM,LlamaTokenizer, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
        "   DataCollatorForLanguageModeling, Trainer, TrainingArguments, TextStreamer\n",
        "from datasets import load_dataset,Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tVw4fuHhpiU"
      },
      "outputs": [],
      "source": [
        "# Loading Model and Tokenizer with a GPU limit of at most 8 GB\n",
        "def load_model(model_name, bnb_config):\n",
        "   n_gpus = torch.cuda.device_count()\n",
        "   max_memory = f'{8000}MB'\n",
        "\n",
        "\n",
        "   model = LlamaForCausalLM.from_pretrained(\n",
        "       model_name,\n",
        "       quantization_config=bnb_config,\n",
        "       device_map=\"auto\",  # Efficiently dispatch the model on available resources\n",
        "       max_memory={i: max_memory for i in range(n_gpus)},\n",
        "       cache_dir=cache_dir\n",
        "   )\n",
        "   tokenizer = LlamaTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
        "\n",
        "\n",
        "   # Needed for LLaMA tokenizer\n",
        "   tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "   return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Org5Zl6h8bS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a BitsAndBytesConfig for quantization\n",
        "def create_bnb_config():\n",
        "   # Configure BitsAndBytes quantization with specific settings\n",
        "   bnb_config = BitsAndBytesConfig(\n",
        "       load_in_4bit=True,                    # Load weights in 4-bit format\n",
        "       bnb_4bit_use_double_quant=True,       # Use double quantization for 4-bit\n",
        "       bnb_4bit_quant_type=\"nf4\",           # 4-bit quantization type\n",
        "       bnb_4bit_compute_dtype=torch.bfloat16, # Compute data type for 4-bit\n",
        "   )\n",
        "\n",
        "\n",
        "   return bnb_config\n",
        "\n",
        "\n",
        "# Create a Parameter-Efficient Fine-Tuning config for your model\n",
        "def create_peft_config(modules):\n",
        "   \"\"\"\n",
        "   Create Parameter-Efficient Fine-Tuning config for your model\n",
        "   :param modules: Names of the modules to apply Lora to\n",
        "   \"\"\"\n",
        "   # Configure Lora (Parameter-Efficient Fine-Tuning) with specific settings\n",
        "   config = LoraConfig(\n",
        "       r=16,                # Dimension of the updated matrices\n",
        "       lora_alpha=64,       # Parameter for scaling\n",
        "       target_modules=modules, # Names of the modules to apply Lora to\n",
        "       lora_dropout=0.05,    # Dropout probability\n",
        "       bias=\"none\",         # Bias type\n",
        "       task_type=\"CAUSAL_LM\", # Task type (Causal Language Modeling in this case)\n",
        "   )\n",
        "\n",
        "\n",
        "   return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr2QK8rxiBXl"
      },
      "outputs": [],
      "source": [
        "def create_prompt_formats(sample):\n",
        "   \"\"\"\n",
        "   Format various fields of the sample ('instruction', 'context', 'response')\n",
        "   Then concatenate them using two newline characters\n",
        "   :param sample: Sample dictionary\n",
        "   \"\"\"\n",
        "\n",
        "\n",
        "   system_prompt = '''तुम एक सहायक हो जो सटीक और संक्षेपित उत्तर प्रदान करता है। कृपया प्रदान किए गए पाठ में सूचना ढूंढ़ें और सवाल का संक्षेप में उत्तर दें। अगर आपको उत्तर नहीं पता है, तो एक से ज्यादा वाक्य में बस बताएं कि आप नहीं जानते।'''\n",
        "\n",
        "\n",
        "   B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "   B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\"\n",
        "\n",
        "\n",
        "   user_prompt = sample['question']\n",
        "   context = sample['context']\n",
        "   response = sample['answer']\n",
        "\n",
        "   prompt = f\"{B_INST} {B_SYS} {system_prompt.strip()} {E_SYS} \\n संदर्भ: {context.strip()} \\n प्रश्न: {user_prompt.strip()} {E_INST} \\n\\n  उत्तर: {response}\"\n",
        "\n",
        "\n",
        "   return prompt\n",
        "\n",
        "\n",
        "def generate_and_tokenize_prompt(data_point):\n",
        "   full_prompt = create_prompt_formats(data_point)\n",
        "   tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True,max_length=1024)\n",
        "   return tokenized_full_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hp-CH4qiGDL"
      },
      "outputs": [],
      "source": [
        "def find_all_linear_names(model):\n",
        "   cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
        "   lora_module_names = set()\n",
        "   for name, module in model.named_modules():\n",
        "       if isinstance(module, cls):\n",
        "           names = name.split('.')\n",
        "           lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "\n",
        "   if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
        "       lora_module_names.remove('lm_head')\n",
        "   return list(lora_module_names)\n",
        "\n",
        "\n",
        "def print_trainable_parameters(model, use_4bit=False):\n",
        "   \"\"\"\n",
        "   Prints the number of trainable parameters in the model.\n",
        "   \"\"\"\n",
        "   trainable_params = 0\n",
        "   all_param = 0\n",
        "   for _, param in model.named_parameters():\n",
        "       num_params = param.numel()\n",
        "       # if using DS Zero 3 and the weights are initialized empty\n",
        "       if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
        "           num_params = param.ds_numel\n",
        "\n",
        "\n",
        "       all_param += num_params\n",
        "       if param.requires_grad:\n",
        "           trainable_params += num_params\n",
        "   if use_4bit:\n",
        "       trainable_params /= 2\n",
        "   print(\n",
        "       f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
        "   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq6sFByqicIt"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"HydraIndicLM/Hindi_Train_ClosedDomainQA\")\n",
        "\n",
        "\n",
        "cache_dir = \"/media/anil/New Volume1/sumedha/OHmodel/model\" # Model Location\n",
        "os.makedirs(cache_dir,exist_ok=True)\n",
        "\n",
        "\n",
        "model_name = \"sarvamai/OpenHathi-7B-Hi-v0.1-Base\"\n",
        "bnb_config = create_bnb_config() # Creating Configuration\n",
        "\n",
        "\n",
        "model, tokenizer = load_model(model_name, bnb_config)\n",
        "\n",
        "\n",
        "training_data = dataset[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQRw50OGnbCg"
      },
      "outputs": [],
      "source": [
        "def train(model, tokenizer, dataset, output_dir):\n",
        "  # Apply preprocessing to the model to prepare it by\n",
        "  # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
        "  model.gradient_checkpointing_enable()\n",
        "\n",
        "\n",
        "  # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
        "  model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "\n",
        "  # Get lora module names\n",
        "  modules = find_all_linear_names(model)\n",
        "  print(modules)\n",
        "\n",
        "\n",
        "  # Create PEFT config for these modules and wrap the model to PEFT\n",
        "  peft_config = create_peft_config(modules)\n",
        "  model = get_peft_model(model, peft_config)\n",
        "\n",
        "\n",
        "  # Print information about the percentage of trainable parameters\n",
        "  print_trainable_parameters(model)\n",
        "\n",
        "\n",
        "  # Training parameters\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      train_dataset=training_data,\n",
        "      args=TrainingArguments(\n",
        "          num_train_epochs=2,\n",
        "          per_device_train_batch_size=4,\n",
        "          gradient_accumulation_steps=4,\n",
        "          max_steps=10,\n",
        "          learning_rate=2e-4,\n",
        "          fp16=True,\n",
        "          lr_scheduler_type =\"cosine\",\n",
        "          logging_steps=10,\n",
        "          warmup_ratio = 0.03,\n",
        "          output_dir=\"outputs\",\n",
        "          optim='paged_adamw_32bit',\n",
        "      ),\n",
        "      data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "  )\n",
        "\n",
        "\n",
        "  model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
        "\n",
        "\n",
        "  ### SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
        "  # Verifying the datatypes before training\n",
        "\n",
        "\n",
        "  dtypes = {}\n",
        "  for _, p in model.named_parameters():\n",
        "      dtype = p.dtype\n",
        "      if dtype not in dtypes: dtypes[dtype] = 0\n",
        "      dtypes[dtype] += p.numel()\n",
        "  total = 0\n",
        "  for k, v in dtypes.items(): total+= v\n",
        "  for k, v in dtypes.items():\n",
        "      print(k, v, v/total)\n",
        "\n",
        "\n",
        "  do_train = True\n",
        "\n",
        "\n",
        "  # Launch training\n",
        "  print(\"Training...\")\n",
        "\n",
        "\n",
        "  if do_train:\n",
        "      train_result = trainer.train()\n",
        "      metrics = train_result.metrics\n",
        "      trainer.log_metrics(\"train\", metrics)\n",
        "      trainer.save_metrics(\"train\", metrics)\n",
        "      trainer.save_state()\n",
        "      print(metrics)\n",
        "\n",
        "\n",
        "  ###\n",
        "\n",
        "\n",
        "  # Saving model\n",
        "  print(\"Saving last checkpoint of the model...\")\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "  trainer.model.save_pretrained(output_dir)\n",
        "\n",
        "\n",
        "  # Free memory for merging weights\n",
        "  # del model\n",
        "  del trainer\n",
        "  torch.cuda.empty_cache()\n",
        "  import gc\n",
        "  gc.collect()\n",
        "  gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWenVqexbYFS"
      },
      "outputs": [],
      "source": [
        "output_dir = \"/content/drive/My Drive/llama2/final_checkpoint\"\n",
        "train(model, tokenizer, dataset, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFZdan61jLl5",
        "outputId": "9603d348-e98e-48ab-ad99-4006fa47741e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1468: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['हाथी को अंग्रेजी में क्या कहते हैं?\\n संतुलित करें।\\n \\n \\n \\n \\n \\n \\n \\n \\n']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "prompt = \"हाथी को अंग्रेजी में क्या कहते हैं?\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate\n",
        "generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
        "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n1rziifJmbXb"
      },
      "outputs": [],
      "source": [
        "!pip install langchain transformers qdrant-client accelerate torch bitsandbytes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7jmb3W9hpHT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9TXZpTeh0v5"
      },
      "outputs": [],
      "source": [
        "INDIC_NLP_RESOURCES = r\"C:\\Users\\sumed\\Desktop\\Projects\\llama_agent\\indic_nlp_resources\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St3PFnmUh39L"
      },
      "outputs": [],
      "source": [
        "from indicnlp import common\n",
        "\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SQyRKCjh7ND"
      },
      "outputs": [],
      "source": [
        "from indicnlp import loader\n",
        "\n",
        "loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIqeuRv1iDm3"
      },
      "outputs": [],
      "source": [
        "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
        "\n",
        "with open(\"test_text.txt\", \"r\") as file:\n",
        "    input_text= file.read()\n",
        "# input_text = \"\"\n",
        "remove_nuktas = False\n",
        "factory = IndicNormalizerFactory()\n",
        "normalizer = factory.get_normalizer(\"hi\")\n",
        "output_text = normalizer.normalize(input_text)\n",
        "\n",
        "print(input_text)\n",
        "print()\n",
        "\n",
        "print(\"Before normalization\")\n",
        "\n",
        "print(\" \".join([hex(ord(c)) for c in input_text]))\n",
        "print(\"Length: {}\".format(len(input_text)))\n",
        "print()\n",
        "print(\"After normalization\")\n",
        "print(\" \".join([hex(ord(c)) for c in output_text]))\n",
        "print(\"Length: {}\".format(len(output_text)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_5S9TukiEeT"
      },
      "source": [
        "तोड़ दिया था और उसके खंडहरों पर दक्षिण में कई राज्य उठ खड़े हुए थे। इससे\n",
        "बहुत पहले, चौदहवीं शताब्दी के आरंभ में, दो बड़े राज्य कायम हुए थे-गुलबर्ग\n",
        "जो बहमनी राज्य के नाम से प्रसिद्ध है और विजयनगर का हिंदू राज्य।\n",
        "\n",
        "दिल्‍ली की तबाही के बाद उत्तरी भारत कमज़ोर पड़कर टुकड़ों में बँट\n",
        "गया। दक्षिण भारत की स्थिति बेहतर थी और वहाँ के राज्यों में सबसे बड़ी\n",
        "और शक्तिशाली रियासत विजयनगर थी। इस रियासत और नगर ने उत्तर के        \n",
        "बहुत से हिंदू शरणार्थियों को आकर्षित किया। उपलब्ध वृत्तांतों से पता चलता\n",
        "है कि शहर बहुत समृद्ध और सुंदर था।\n",
        "\n",
        "जब दक्षिण में विजयनगर तरक्की कर रहा था, उस समय उत्तर की\n",
        "पहाड़ियों से होकर एक और हमलावर दिल्ली के पास, पानीपत के प्रसिद्ध\n",
        "मैदान में आया। उसने 526 ई. में दिल्‍ली के सिंहासन को जीत लिया। मध्य\n",
        "एशिया के तैमूर वंश का यह तुर्क-मंगोल बाबर था। भारत में मुगल साम्राज्य\n",
        "की नींव उसी ने डाली।\n",
        "\n",
        "समन्वय और मिली-जुली संस्कृति का विकास\n",
        "कबीर, गुरु नानक और अमीर खुसरो\n",
        "\n",
        "भारत पर मुस्लिम आक्रमण की या भारत में मुस्लिम युग की बात करना\n",
        "गलत और भ्रामक है। इस्लाम ने भारत पर आक्रमण नहीं किया, वह भारत\n",
        "में कुछ सदियों के बाद आया। आक्रमण तुर्कों (महमूद) ने किया था,\n",
        "अफ़गानों ने किया था और उसके बाद तुर्क-मंगोल या मुगल आक्रमण\n",
        "हुआ। इनमें से बाद के दो आक्रमण महत्त्वपूर्ण थे। अफ़गानों को हम भारत\n",
        "का सीमावर्ती समुदाय कह सकते हैं, जो भारत के लिए पूरी तरह अजनबी\n",
        "भी नहीं माने जा सकते। उनके राजनीतिक शासन के काल को हिंद-\n",
        "अफ़गान युग कहना चाहिए। मुगल भारत के लिए बाहर के और अजनबी\n",
        "लोग थे, फिर भी वे भारतीय ढाँचे में बड़ी तेज़ी से समा गए और उन्होंने\n",
        "हिंद-मुगल युग की शुरुआत की।\n",
        "\n",
        "अफ़गान शासक और जो लोग उनके साथ आए थे वे भी भारत में समा\n",
        "गए। उनके परिवारों का पूरी तरह भारतीयकरण हो गया। भारत को वे अपना\n",
        "\n",
        "नयी समस्याएँ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bEsjtRziLma"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def remove_long_spaces(text):\n",
        "    # Define a regular expression pattern to match spaces longer than one newline\n",
        "    pattern = re.compile(r\" {2,}\")\n",
        "\n",
        "    # Replace the matched pattern with a single space\n",
        "    return re.sub(pattern, \" \", text)\n",
        "\n",
        "mod_input_text=remove_long_spaces(input_text)\n",
        "print(mod_input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUWpyqH0iPzb"
      },
      "outputs": [],
      "source": [
        "from indicnlp.tokenize import sentence_tokenize\n",
        "\n",
        "\n",
        "sentences = sentence_tokenize.sentence_split(mod_input_text, lang=\"hi\")\n",
        "for t in sentences:\n",
        "    print(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QzgCwgniUHJ"
      },
      "outputs": [],
      "source": [
        "import fasttext as ft\n",
        "\n",
        "# Loding model for Hindi.\n",
        "embed_model = ft.load_model(\"wiki.hi.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GvAEdi1iZ9h"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        " chunk_size=1000,\n",
        " chunk_overlap=20,\n",
        " length_function=len,\n",
        " is_separator_regex=False,\n",
        ")\n",
        "text_path = \"C:\\\\Users\\\\sumed\\\\Desktop\\\\Projects\\\\llama_agent\"\n",
        "loader = DirectoryLoader(text_path, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
        "docs = loader.load_and_split(text_splitter=text_splitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qj0pNGW-if1d"
      },
      "outputs": [],
      "source": [
        "import fasttext as ft\n",
        "\n",
        "# Loding model for Hindi.\n",
        "embed_model = ft.load_model(\"wiki.hi.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgJdS8LvijPg"
      },
      "outputs": [],
      "source": [
        "df[\"embeddings\"] = df[\"page_content\"].apply(\n",
        "    lambda x: (embed_model.get_sentence_vector(x)).tolist()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyJUPFdSioCA"
      },
      "outputs": [],
      "source": [
        "df[\"id\"] = range(1, len(df) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr9SZ7c1iriQ"
      },
      "outputs": [],
      "source": [
        "payload = df[[\"page_content\", \"metadata\"]].to_dict(orient=\"records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knE266_ziuVS"
      },
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amykGIMRixyZ"
      },
      "outputs": [],
      "source": [
        "from qdrant_client.http import models\n",
        "\n",
        "client.delete_collection(collection_name=\"hindi_collection\")\n",
        "client.create_collection(\n",
        "    collection_name=\"hindi_collection\",\n",
        "    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KpbpFIli0wb"
      },
      "outputs": [],
      "source": [
        "client.upsert(\n",
        "    collection_name=\"hindi_collection\",\n",
        "    points=models.Batch(\n",
        "        ids=df[\"id\"].to_list(),\n",
        "        payloads=payload,\n",
        "        vectors=df[\"embeddings\"].to_list(),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mst20Ad8i9BS"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "# preparing config for quantizing the model into 4 bits\n",
        "# quantization_config = BitsAndBytesConfig(\n",
        "#  load_in_4bit=True,\n",
        "#  bnb_4bit_compute_dtype=torch.float16,\n",
        "#  bnb_4bit_quant_type=\"nf4\",\n",
        "#  bnb_4bit_use_double_quant=True,\n",
        "# )\n",
        "# load the tokenizer and the quantized mistral model\n",
        "# model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "# model_4bit = AutoModelForCausalLM.from_pretrained(\n",
        "#  model_id,\n",
        "#  device_map=\"auto\",\n",
        "#  quantization_config=quantization_config,)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "# using HuggingFace's pipeline\n",
        "pipeline = pipeline(\n",
        " \"text-generation\",\n",
        " model=model_4bit,\n",
        " tokenizer=tokenizer,\n",
        " use_cache=True,\n",
        " device_map=\"auto\",\n",
        " max_new_tokens=5000,\n",
        " do_sample=True,\n",
        " top_k=1,\n",
        " temperature = 0.01,\n",
        " num_return_sequences=1,\n",
        " eos_token_id=tokenizer.eos_token_id,\n",
        " pad_token_id=tokenizer.eos_token_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bif3oyqwjVjr"
      },
      "outputs": [],
      "source": [
        "def generate_text(question):\n",
        " # Searching for relevant hits in the 'speech_collection'\n",
        " hits = client.search(\n",
        " collection_name=\"hindi_collection\",\n",
        " query_vector= embed_model.get_sentence_vector(question).tolist(),\n",
        " limit=10,\n",
        " )\n",
        " # Creating context from the hits\n",
        " context = ''\n",
        " for hit in hits:\n",
        "    context += hit.payload['page_content'] + '\\n'\n",
        " # Constructing the prompt\n",
        " prompt = f\"\"\"<s>[INST] आप एक सम्मानीय सहायक हैं। आपका काम नीचे दिए गए संदर्भ से प्रश्नों का उत्तर देना है।\n",
        " संदर्भ: {context}\n",
        " प्रश्न: {question} [/INST] </s>\n",
        " \"\"\"\n",
        " # Generating text using the GPT model\n",
        " sequences = pipeline(\n",
        " prompt,\n",
        " do_sample=True,\n",
        " temperature=0.7,\n",
        " top_k=50,\n",
        " top_p=0.95,\n",
        " num_return_sequences=1,\n",
        " )\n",
        " return sequences[0]['generated_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1N3Wgazjgk0"
      },
      "outputs": [],
      "source": [
        "generate_text(\"मुझे गांधी के बारे में विस्तार से बताएं।\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}